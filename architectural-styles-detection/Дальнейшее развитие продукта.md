# Дальнейшее развитие продукта
<p>&nbsp;</p>

## Оглавление

1. [Дальнейшее развитие продукта](#дальнейшее-развитие-продукта)
   - [Возможности текущей модели](#возможности-текущей-модели)
   - [Ограничения модели](#ограничения-модели)
   - [Расширение возможностей за счёт собственной среды](#расширение-возможностей-за-счёт-собственной-среды)
     - [Преимущества использования Roboflow](#преимущества-использования-roboflow)
     - [Недостатки использования Roboflow](#недостатки-использования-roboflow)

2. [Реализация проекта при развёртывании своей модели](#реализация-проекта-при-развёртывании-своей-модели)
   - [Преимущества самостоятельного развёртывания модели](#преимущества-самостоятельного-развёртывания-модели)
   - [Масштабирование проекта](#масштабирование-проекта)
   - [Тонкая настройка модели](#тонкая-настройка-модели)

3. [План реализации проекта при развёртывании собственных сервисов](#план-реализации-проекта-при-развёртывании-собственных-сервисов)
   - [1. Определение требований и целей](#1-определение-требований-и-целей)
   - [2. Сбор и подготовка данных](#2-сбор-и-подготовка-данных)
     - [Сбор данных](#сбор-данных)
     - [Разметка данных](#разметка-данных)
     - [Подготовка датасета](#подготовка-датасета)
     - [Встраивание продвинутых методов аугументации](#встраивание-продвинутых-методов-аугументации)
   - [3. Выбор и обучение модели](#3-выбор-и-обучение-модели)
     - [Выбор архитектуры модели](#выбор-архитектуры-модели)
     - [Обучение модели](#обучение-модели)
     - [Визуализации графиков обучения](#визуализации-графиков-обучения)
     - [Оценка модели](#оценка-модели)
   - [4. Развёртывание сервиса, тестирование, оптимизация, поддержка](#4-развёртывание-сервиса-тестирование-оптимизация-поддержка)
   - [5. Развитие проекта: будущие возможности, требуемые ресурсы и технологии](#5-развитие-проекта)
<p>&nbsp;</p>   
       
## Возможности текущей модели 

Использованная в данном проекте модель машинного обучения ArchiStyles была изначально обучена на 9 стилях. Для того, чтобы продемонстрировать возможности развития модели, она дополнительно была дообучена командой на ещё двух стилях (Чикагская школа и Эдвардианский стиль). Таким образом, разработанная модель может определять 11 распространенных в Западной и Центральной Европе архитектурных стилей:

1. Art Nouveau (Модерн)
2. Baroque (Барокко)
3. Beaux-Arts (Бозар)
4. Byzantine (Византийская архитектура)
5. Chicago School (Чикагская архитектурная школа)
6. Deconstructivism (Деконструктивизм)
7. Edwardian (Эдвардианская архитектура)
8. Gothic (Готика)
9. Palladian (Палладианство)
10. Postmodern (Постмодернизм)
11. Romanesque (Романская архитектура)

Выбор данных стилей был связан с доступностью этих наборов данных для обучения модели. При наличии ресурсов (в первую очередь, людей в команде и времени), более глубоко погружения в процесс сбора данных и выполнения ряда дополнительных шагов, модель может быть существенно доработана.

---
<p>&nbsp;</p>

## Ограничения модели

Актуальная версия модели имеет ряд ограничений, которые можно устранить при переходе к собственной среде разработки и более глубокой настройке модели:

### 1. Ошибки в определении стиля
Точность модели может снижаться при определении архитектурного стиля из-за недостаточной оптимизации алгоритмов и объёма данных.

**Решение:** Перейти к собственной среде разработки, где мы сможем:
- точнее настраивать гиперпараметры модели,
- использовать свою аугментацию данных, которая может быть настроена более точно, чем встроенная в модели Roboflow, т.к. она будет учитывать специфику нашего типа изображений.
- экспериментировать с архитектурами (YOLOv5, ResNet, EfficientNet и др.).

### 2. Ограниченный список стилей
В текущей версии модель распознаёт только обученные 11 стилей.

**Решение:** В собственной среде можно расширить список стилей и добавить географическую локализацию, обучив модель на более репрезентативном и разнообразном датасете (с учётом особенностей нашей архитектуры).

### 3. Работа со смешанными стилями
Модель испытывает трудности при анализе зданий с элементами нескольких стилей (например, эклектика, фьюжн).

**Решение:** 
- адаптировать модель для работы с вероятностными предсказаниями,
- изменить формат вывода так, чтобы модель показывала распределение вероятностей по стилям. В таких случаях более корректным было бы ответить, что здание на фото относится к тем или иным стилям с некоторой вероятностью. Например:
> "Здание имеет признаки нескольких стилей: 60% — модерн, 40% — постмодернизм."

---
<p>&nbsp;</p>

## Расширение возможностей за счёт собственной среды

Переход к собственной среде разработки и развертыванию модели YOLOv5 без Roboflow решит текущие ограничения и даст больше контроля над проектом. YOLOv5 — это полноценная модель машинного обучения, которая может работать автономно без инструментов-посредников.


### Преимущества использования Roboflow:
1. Удобный интерфейс для разметки изображений и создания датасетов.
2. Автоматическая аугментация данных.
3. Экспорт данных в различных форматах (YOLO, COCO, Pascal VOC), что делает его совместимым с большим количеством моделей машинного обучения.
4. Хостинг датасетов и моделей в облаке, предоставление к ним удобного доступа через API.
5. Прямая интеграция с YOLOv5, что значительно упрощает процесс обучения и развертывания модели.


### Недостатки использования Roboflow:
- Ограничение функционала платформы.
- Зависимость от сторонних сервисов.

В целом, использование Roboflow значительно упрощает разработку проекта, особенно при отсуствии большого опыта в области машинного обучения. При наличии необходимых знаний и опыта, проект можно реализовать и без использования Roboflow, и это существенно расширит возможности изменения настроек модели под нужды, цели и задачи авторов проекта.

---
<p>&nbsp;</p>

## Реализация проекта при развёртывании своей модели 

При самостоятельном развертывании модели, открывает ряд возможностей, недоступных при использовании готовых платформ.

### Преимущества самостоятельного развёртывания модели:
1. Полный контроль над обучением: выбор архитектуры модели, настройка гиперпараметров и методов аугментации.
2. Использование любых данных: возможность работы с нестандартными датасетами и комбинирования разных датасетов.
3. Глубокое понимание ML-процессов: самостоятельная реализация проекта дает возможность глубже понять принципы работы моделей машинного обучения, тонкости настройки гиперпараметров и выбора оптимальной архитектуры..
4. Свобода в развертывании: возможность интеграции модели в различные среды.
5. Оптимизация производительности: повышение скорости инференса и уменьшение зависимости от ресурсов.
<p>&nbsp;</p>

### Масштабирование проекта

Самостоятельное развертывание модели откроеn возможности для масштабирования проекта и позволит расширить функциональность. В целом, самостоятельное развертывание модели даёт полный контроль над проектом и позволяет масштабировать его в соответствии с целями и ресурсами, но требует больше усилий, технических знаний и времени.

**1. Масштабирование датасета:**
- Увеличение объёма данных для повышения точности модели.
- Добавление новых стилей, включая редкие и регионально-специфичные.
- Создание специализированных моделей для отдельных городов.

**2. Масштабирование функциональности:**
- Эксперименты с архитектурой моделей позволят выбирать и настраивать архитектуру модели (например, YOLOv5), оптимизировать её под задачи.
- Реализация multi-label классификации - определение нескольких стилей на изображении.
- Интеграция с любыми приложениями (веб-сервисы, мобильные приложения, Telegram-боты).
- Оптимизация производительности (настройка параметров для увеличения скорости работы).
<p>&nbsp;</p>

### Тонкая настройка модели

Для повышения производительности модели может быть произведена точная настройка следующих параметров:
1. Скорость обучения (learning rate): определяет, как быстро модель обновляет веса.
2. Размер батча (batch size): количество изображений на одну итерацию.
3. Количество эпох (epochs): количество проходов по всему набору данных.
4. Оптимизатор (optimizer): алгоритм для обновления весов.
5. Архитектура модели: выбор структуры модели для оптимизации, ускорения обучения и повышения точности, например, использование предобученных весов.

---
<p>&nbsp;</p>

# План реализации проекта при развёртывании собственных сервисов
<p>&nbsp;</p>

## 1. Определение требований и целей

- **Четкое определение целей проекта**:
  - Архитектурные стили, которые нужно определять.
  - Требуемая точность модели.
  - Необходимая скорость инференса.

- **Анализ целевой аудитории**:
  - Как пользователи будут взаимодействовать с моделью: через веб-интерфейс, мобильное приложение или API.
  - Интеграция с другими сервисами.

- **Оценка доступных ресурсов**:
  - Финансовый бюджет.
  - Наличие команды специалистов.
  - Доступные вычислительные мощности.
<p>&nbsp;</p>

## 2. Сбор и подготовка данных

### Сбор данных

- Собрать разнообразный набор изображений, представляющих различные архитектурные стили.
- Использовать различные источники:
  - Фотостоки.
  - Архитектурные сайты.
  - Социальные сети.
    

### Разметка данных

- Выбор инструментов для ручной разметки: LabelImg, CVAT, makesense.ai.
- Разметка изображений:
  - Указать bounding box для каждого здания.
  - Присвоить изображению соответствующий архитектурный стиль.
- Качество разметки:
  - Обеспечить точность, полноту и согласованность разметки.
- Рассмотреть возможность использования сервисов автоматической разметки для ускорения процесса.

Пример кода для генерации разметки в формате YOLO
```python
import os

# Пути к данным
data_dir = "/content/drive/MyDrive/Хакатон/data/data_for_model"  # Общая папка с данными
splits = ['train', 'valid', 'test']  # Три раздела данных

# Список классов и их ID
classes = sorted([d for d in os.listdir(os.path.join(data_dir, 'train')) if os.path.isdir(os.path.join(data_dir, 'train', d))])
class_to_id = {cls_name: i for i, cls_name in enumerate(classes)}
print(f"Классы и их ID: {class_to_id}")

# Генерация разметки для всех сплитов
for split in splits:
    split_path = os.path.join(data_dir, split)
    print(f"Создаём разметку для: {split}")
    for class_name in classes:
        class_path = os.path.join(split_path, class_name)
        for img_name in os.listdir(class_path):
            if img_name.lower().endswith(('.jpg', '.jpeg', '.png')):
                # Создаём файл аннотации рядом с изображением
                label_file = os.path.join(class_path, f"{os.path.splitext(img_name)[0]}.txt")
                with open(label_file, 'w') as f:
                    f.write(f"{class_to_id[class_name]} 0.5 0.5 1.0 1.0\n")  # YOLO формат для классификации
    print(f"Разметка для {split} создана успешно!")
```

В результате мы получим текстовые файлы .txt рядом с каждым изображением, каждый файл внутри содержит строку:
```bash
<class_id> 0.5 0.5 1.0 1.0
```
![Архитектурные стили](https://drive.google.com/uc?export=view&id=1pd9RCwZ328EOTrKt94P6RR4WTUf6yjkk)


### Подготовка датасета

Подготовка данных подразумевает выполнение шагов:
- Разделить данные на обучающую, валидационную и тестовую выборки.
- Сохранить данные в формате, совместимом с выбранным фреймворком машинного обучения.

```python
# Установка библиотеки
pip install split-folders

# Можно использовать библиотеку shutil и splitfolders для автоматического разделения данных
import splitfolders

# Исходные данные
input_folder = "/content/drive/MyDrive/Хакатон/data/data_for_model/original_data"  # Исходная папка с изображениями по классам

# Сохранение разделённых данных
output_folder = "/content/drive/MyDrive/Хакатон/data/data_for_model"

# Разделение на train, valid, test (80%, 10%, 10%)
splitfolders.ratio(input_folder, output=output_folder, seed=42, ratio=(0.8, 0.1, 0.1), move=True)

print("Данные успешно разделены на train, valid и test!")
```

В результате данные будут распределены по папкам с такой структурой:
```bash
output_folder/
    train/
        class1/
        class2/
    valid/
        class1/
        class2/
    test/
        class1/
        class2/
```

Данные сохраняются в формате, совместимом с YOLOv5. Для YOLOv5 создаётся файл data.yaml, который укажет пути к данным, количество классов и их названия.
```python
import os

# Пути к разделённым данным
train_path = "/content/drive/MyDrive/Хакатон/data/data_for_model/train"
valid_path = "/content/drive/MyDrive/Хакатон/data/data_for_model/valid"

# Список классов
classes = sorted(os.listdir(train_path))

# Создаём YAML-содержимое
data_yaml = f"""
train: {train_path}
valid: {valid_path}
nc: {len(classes)}
names: {classes}
"""

# Сохраняем файл data.yaml
yaml_path = os.path.join("/content/drive/MyDrive/Хакатон/data/data_for_model", "data.yaml")
with open(yaml_path, "w") as f:
    f.write(data_yaml)

print(f"Файл data.yaml успешно создан! Путь: {yaml_path}")
print(data_yaml)
"""
```

В результате будет создан файл data.yaml с содержимым:
```bash
train: /content/drive/MyDrive/Хакатон/data/data_for_model/train
valid: /content/drive/MyDrive/Хакатон/data/data_for_model/valid
nc: 2
names: ['Gothic', 'Baroque']
```
![Папки с данными](https://drive.google.com/uc?export=view&id=17LLRRxLC0Z3CYY0pEB0KcUY-3XjdMgvi)

### Встраивание продвинутых методов аугументации

Как было упомянуто выше, для повышения эффективности модели могут быть использованы более продвинутые методы аугментации данных. Они помогут улучшить способность модели к генерализации, особенно при работе с изображениями разного качества и условий съёмки. Реализация может быть выполнена с использованием библиотек Albumentations, OpenCV или PyTorch. 
```python
# Подключение новых библиотек аугментации (Albumentations или OpenCV)
import albumentations as A
from albumentations.pytorch import ToTensorV2
import cv2
from torch.utils.data import Dataset, DataLoader
```

Могут быть использованы следующие техники аугументации:

**1. Геометрические преобразования:**

Преобразования, изменяющие геометрию изображения, помогут имитировать разные ракурсы зданий и их пропорции.
- Отражение (flip): Отражение изображения по горизонтали или вертикали.
- Сдвиг (shift): Сдвиг изображения по вертикали или горизонтали.
- Масштабирование (scale): Увеличение или уменьшение изображения.
- Поворот (rotate): Ротация на произвольный угол, а не только 90°.
- Искажение перспективы (perspective): Симуляция изменения угла обзора.

Реализация (Python + Albumentations):
```python
import albumentations as A
from albumentations.pytorch import ToTensorV2
import cv2

transform = A.Compose([
    A.HorizontalFlip(p=0.5),
    A.ShiftScaleRotate(shift_limit=0.05, scale_limit=0.1, rotate_limit=45, p=0.5),
    A.Perspective(scale=(0.05, 0.1), p=0.3),
    ToTensorV2(),
])

# Применения трансформации
image = cv2.imread("example.jpg")
augmented = transform(image=image)
augmented_image = augmented["image"]
```

**2. Цветовые изменения:**

Такие преобразования помогут модели справляться с изменением освещения и цветовых оттенков.
- Изменение яркости и контрастности (brightness/contrast): Эмуляция различного освещения.
- Изменение гаммы (gamma correction): Имитирование оттенков на фото.
- Преобразование в чёрно-белый режим.
- Шум (noise): Добавление случайного шума, чтобы имитировать низкокачественные снимки.

```python
transform = A.Compose([
    A.RandomBrightnessContrast(p=0.5),
    A.HueSaturationValue(hue_shift_limit=20, sat_shift_limit=30, valid_shift_limit=20, p=0.5),
    A.GaussNoise(var_limit=(10.0, 50.0), p=0.3),
    ToTensorV2(),
])

image = cv2.imread("example.jpg")
augmented = transform(image=image)
augmented_image = augmented["image"]
```

**3. Обрезка (Cropping):**

Модель может учиться на частях изображений, что улучшает её способность выделять важные элементы стиля.
- Случайное обрезание (random crop): Удаление частей изображения.
- Фиксированное обрезание (center crop): Увеличение интереса к центральной части здания.

```python
transform = A.Compose([
    A.RandomCrop(height=256, width=256, p=0.5),
    ToTensorV2(),
])
```

**4. Аугментация на уровне шума:**

Добавление визуальных дефектов, которые встречаются на реальных изображениях.
- Добавление размытия (blur): Симуляция движения камеры.
- Добавление дождя, снега, тумана (weather augmentation).
- Артефакты JPEG: Имитирование изображений низкого качества.

```python
transform = A.Compose([
    A.MotionBlur(blur_limit=3, p=0.3),
    A.ImageCompression(quality_lower=30, quality_upper=60, p=0.3),
    A.Fog(fog_coef_lower=0.1, fog_coef_upper=0.3, p=0.2),
    ToTensorV2(),
])
```

**5. Смешивание изображений (CutMix, MixUp):**

Эти методы полезны, если здания на изображениях принадлежат к нескольким стилям.
- CutMix: Смешивание частей двух изображений.
- MixUp: Линейная интерполяция между двумя изображениями.

**6. Генеративные методы:**
Создание новых изображений с помощью нейросетей.
- Использование StyleGAN для генерации новых изображений зданий.
- ControlNet: для генерации зданий с контролем стиля.
- Инструменты: Stable Diffusion, GAN-архитектуры.

Дополнительные методы аугументации должны применяться только к изображениям из обучающего набора (набор train):
```python
# Создание аугментаций
train_transform = A.Compose([
    A.HorizontalFlip(p=0.5),  # Отражение
    A.ShiftScaleRotate(shift_limit=0.05, scale_limit=0.1, rotate_limit=30, p=0.5),  # Сдвиг, масштабирование, поворот
    A.RandomBrightnessContrast(p=0.3),  # Яркость и контрастность
    A.HueSaturationValue(p=0.3),  # Изменение оттенков
    A.GaussNoise(var_limit=(10.0, 50.0), p=0.3),  # Шум
    A.MotionBlur(blur_limit=3, p=0.2),  # Размытие
    ToTensorV2(),  # Конвертация в формат Tensor
])

# Валидация и тест без аугментации
valid_transform = A.Compose([
    A.Resize(224, 224),  # Изменение размера
    ToTensorV2(),
])
```

При этом потребуется модификация датасета для применения аугментации:
```python
class ArchiStylesDataset(Dataset):
    def __init__(self, image_paths, labels, transform=None):
        self.image_paths = image_paths
        self.labels = labels
        self.transform = transform

    def __len__(self):
        return len(self.image_paths)

    def __getitem__(self, idx):
        image_path = self.image_paths[idx]
        image = cv2.imread(image_path)
        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)  # Конвертация в RGB
        label = self.labels[idx]
        
        # Применяем аугментации
        if self.transform:
            augmented = self.transform(image=image)
            image = augmented['image']
        
        return image, label
```
---
<p>&nbsp;</p>

## 3. Выбор и обучение модели


### Выбор архитектуры модели

Исходя из требований к точности, скорости инференса и доступным ресурсам, выбрать архитектуру модели: например, YOLOv5, ResNet, EfficientNet, ViT.
Мы выбрали YOLOv5 Classification для распознавания архитектурных стилей, т.к. она достаточно лёгкая и быстрая, имеет хорошую точность для классификации изображений, и что немаловажно, по ней можно найти большое количество описаний в интеренете и примеров использования (на случай возникновения сложностей при реализации).
- Клонирование YOLOv5 и установка зависимостей:
```python
# Используем предобученную модель yolov5s-cls.pt
--model yolov5s-cls.pt

# Клонирование YOLOv5 и установка зависимостей
!git clone https://github.com/ultralytics/yolov5
%cd yolov5
!pip install -qr requirements.txt  # Установка зависимостей
```
<p>&nbsp;</p>

### Обучение модели 

Обучение включает следующие этапы:
- Выбрать фреймворк машинного обучения: TensorFlow, PyTorch.
- Настроить гиперпараметры модели: количество эпох, размер батча, скорость обучения и др.
- Интеграция в процесс обучения дополнительных методов ауугументации, если они используются.
- Обучить модель на подготовленном датасете, используя выбранный фреймворк и вычислительные мощности.
- Отслеживать метрики обучения: точность, потери, для оценки качества модели и настройки гиперпараметров.
- Рассмотреть возможность использования предобученных моделей и дообучения их на датасете ArchiStyles (Fine-tuning).

Мы используем PyTorch, на котором основан YOLOv5. 

Настраиваем гиперпараметры:
- epochs: Количество эпох обучения (10).
- batch-size: Размер батча (64).
- img: Размер изображений (128x128 пикселей).

Для интеграции новых методов аугментации в процесс обучения в вашем коде нужно внести изменения 

Интегририруем продвинутую аугментацию в процесс обучения в части кода для подготовки данных перед запуском команды обучения.
- Создаём новые функции для аугментации и используем их при формировании train_loader и valid_loader.
- Добавляем их перед запуском команды обучения YOLOv5 (перед строкой os.system(train_command)).
  
```python
import os
import cv2
import albumentations as A
from albumentations.pytorch import ToTensorV2
from torch.utils.data import DataLoader, Dataset

# Данные и конфигурации
data_yaml = "/content/drive/MyDrive/Хакатон/data/data_for_model/data.yaml"  # Путь к data.yaml
weights_path = "yolov5s-cls.pt"  # Предобученные веса для классификации (YOLOv5s)
epochs = 10  # Количество эпох обучения
batch_size = 64  # Размер батча
img_size = 128  # Размер изображений

# Функции для аугментации
train_transform = A.Compose([
    A.HorizontalFlip(p=0.5),
    A.ShiftScaleRotate(shift_limit=0.05, scale_limit=0.1, rotate_limit=30, p=0.5),
    A.RandomBrightnessContrast(p=0.3),
    A.HueSaturationValue(p=0.3),
    A.GaussNoise(var_limit=(10.0, 50.0), p=0.3),
    A.MotionBlur(blur_limit=3, p=0.2),
    ToTensorV2()
])

valid_transform = A.Compose([
    A.Resize(img_size, img_size),
    ToTensorV2()
])

# Определяем датасет
class ArchiStylesDataset(Dataset):
    def __init__(self, image_paths, labels, transform=None):
        self.image_paths = image_paths
        self.labels = labels
        self.transform = transform

    def __len__(self):
        return len(self.image_paths)

    def __getitem__(self, idx):
        image_path = self.image_paths[idx]
        image = cv2.imread(image_path)
        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)  # Конвертация в RGB
        label = self.labels[idx]

        # Применяем аугментацию
        if self.transform:
            augmented = self.transform(image=image)
            image = augmented['image']

        return image, label

# Функция загрузки данных
def load_images_and_labels(data_split):
    # Загрузка данных (вместо "data_split" будут train/valid/test)
    data_dir = f"/content/drive/MyDrive/Хакатон/data/data_for_model/{data_split}"
    image_paths, labels = [], []
    for class_id, class_name in enumerate(sorted(os.listdir(data_dir))):
        class_dir = os.path.join(data_dir, class_name)
        if os.path.isdir(class_dir):
            for img_file in os.listdir(class_dir):
                if img_file.endswith(('.jpg', '.jpeg', '.png')):
                    image_paths.append(os.path.join(class_dir, img_file))
                    labels.append(class_id)
    return image_paths, labels

# Подготовка данных для обучения модели: загрузка, преобразование, аугментация и формирование батчей (batch)

# Загрузка
train_images, train_labels = load_images_and_labels('train')
valid_images, valid_labels = load_images_and_labels('valid')

# Датасеты: получаем список путей к изображениям (train_images или valid_images), список меток (train_labels или valid_labels), аугментации (transform)
# Они будут применены к каждому изображению: train_transform для обучающих, valid_transform для валидационных данных
train_dataset = ArchiStylesDataset(train_images, train_labels, transform=train_transform)
valid_dataset = ArchiStylesDataset(valid_images, valid_labels, transform=valid_transform)

# Создание DataLoader: деление данных на группы изображений (на батчи), которые передаются в модель за один шаг обучения
train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)
valid_loader = DataLoader(valid_dataset, batch_size=batch_size, shuffle=False)

# Команда для запуска обучения
train_command = f"""
python classify/train.py \
  --model {weights_path} \
  --data {data_yaml} \
  --epochs {epochs} \
  --batch-size {batch_size} \
  --img {img_size} \
  --device 0
"""

# Запуск обучения
print("Запуск обучения модели...")
os.system(train_command)
"""
```

В результате будет создана папка с результатами обучения, где будут сохранены веса модели:
weights/best.pt — веса модели с наилучшей точностью. weights/last.pt — последние веса после завершения всех эпох.
```bash
yolov5/runs/train-cls/exp/
```

Также будут выведены метрики обучения:
- Лог потерь (train loss, validation loss).
- Точность (Top-1, Top-5 accuracy).

Для оценки эффективности обучения можно вывести графики с результатами, достигнутыми в каждой эпохе. На них можно будет видеть, как с каждой следующей эпохой растёт точность предсказаний, которые выдаёт модель.
<p>&nbsp;</p>

### Визуализации графиков обучения
```python
import pandas as pd
import matplotlib.pyplot as plt
import os

# Путь к результатам обучения YOLOv5
results_path = "runs/train-cls/exp/results.csv"
output_dir = "images"  # Папка для сохранения графиков
os.makedirs(output_dir, exist_ok=True)  # Создаём папку, если её нет

# Проверяем существование файла
if not os.path.exists(results_path):
    raise FileNotFoundError(f"Файл с результатами не найден: {results_path}")

# Загрузка данных из CSV
results = pd.read_csv(results_path)

# Очистка названий колонок от лишних пробелов
results.columns = results.columns.str.strip()

# Извлечение данных для построения графиков
epochs = results['epoch']
accuracy_top1 = results['metrics/accuracy_top1'] * 100  # Переводим в проценты
accuracy_top5 = results['metrics/accuracy_top5'] * 100

# Определение максимального значения для Top-1
max_accuracy_top1 = accuracy_top1.max()
max_epoch_top1 = epochs[accuracy_top1.idxmax()]

# Определение максимального значения для Top-5
max_accuracy_top5 = accuracy_top5.max()
max_epoch_top5 = epochs[accuracy_top5.idxmax()]

# Построение графика точности Top-5
plt.figure(figsize=(10, 6))
plt.plot(epochs, accuracy_top5, label='Точность Top-5', marker='o')
plt.scatter(max_epoch_top5, max_accuracy_top5, color='red', s=50, zorder=5,
            label=f"Max: {max_accuracy_top5:.1f}% на эпохе {max_epoch_top5}")
plt.xlabel("Эпоха обучения модели")
plt.ylabel("Точность Top-5 (%)")
plt.title("Точность Top-5 по эпохам")
plt.legend()
plt.grid()
plt.savefig(os.path.join(output_dir, 'top-5_accuracy.jpg'))  # Сохраняем график
plt.show()

# Построение графика точности Top-1
plt.figure(figsize=(10, 6))
plt.plot(epochs, accuracy_top1, label='Точность Top-1', marker='o')
plt.scatter(max_epoch_top1, max_accuracy_top1, color='red', s=50, zorder=5,
            label=f"Max: {max_accuracy_top1:.1f}% на эпохе {max_epoch_top1}")
plt.xlabel("Эпоха обучения модели")
plt.ylabel("Точность Top-1 (%)")
plt.title("Точность Top-1 по эпохам")
plt.legend()
plt.grid()
plt.savefig(os.path.join(output_dir, 'top-1_accuracy.jpg'))  # Сохраняем график
plt.show()
```
<p>&nbsp;</p>

График результатов обучения будет иметь следующий вид:
![График результатов обучения](https://drive.google.com/uc?export=view&id=1-AGBKXuRAwmW8wBkUuk5J6sBBvjRuoqD)


### Оценка модели

После обучения нужно проверить модель на валидационной выборке с использованием сохранённых весов. В результате будут выведены метрики качества модели на валидационных данных (Top-1 и Top-5 accuracy).
```python
# Команда для проверки модели
valid_command = f"""
python classify/valid.py \
  --weights runs/train-cls/exp/weights/best.pt \
  --data {data_yaml}
"""

# Запуск валидации
print("Запуск валидации модели...")
os.system(valid_command)
```

Модель, прошедшую обучение и ваилдацию, проверяем на тестовой выборке, анализируем ошибки, чтобы понять, что можно улучшить.

- Провести эксперименты с различными архитектурами моделей, гиперпараметрами и методами аугментации данных для достижения наилучшей точности.
- Рассмотреть возможность использования ансамблей моделей или комбинирования моделей классификации и обнаружения объектов.

```python
# Путь к папке с тестовыми изображениями
test_path = "/content/drive/MyDrive/Хакатон/data/data_for_model/test"

# Команда для предсказания
predict_command = f"""
python classify/predict.py \
  --weights runs/train-cls/exp/weights/best.pt \
  --source {test_path} \
  --img {img_size}
"""

# Запуск предсказания
print("Запуск предсказания на тестовых изображениях...")
os.system(predict_command)
```

Результаты будут сохранены в папке yolov5/runs/predict-cls/:
- Файл с лучшими весами: best.pt.
- Логи обучения и графики метрик.

---
<p>&nbsp;</p>

## 4. Развёртывание сервиса, тестирование, оптимизация, поддержка

Все последующие этапы являются стандартными неспецифическими для работы только с ML-сервисами.

### Разработка API:
- Создание API, которое принимает изображения на вход и возвращает предсказания модели в удобном формате.
- Использовать подходящий фреймворк: FastAPI, Flask, Django.


### Разработка пользовательского интерфейса:
- Создание пользовательского интерфейса для взаимодействия с моделью: веб-интерфейс, мобильное приложение.
- Обеспечение удобной загрузки изображений, отображения результатов и другие дополнительные функции (сохранение истории, поиск по стилям и т.д.).


### Развёртывание сервиса:
- Выбор платформы для развертывания: собственный сервер, облачные сервисы.
- Настройка серверного окружения: операционная система, веб-сервер, база данных, если необходимо.
- Обеспечить безопасность сервиса: защита от DDoS-атак, авторизация, шифрование данных.


### Тестирование:
- Тестирование сервиса с различными типами изображений: разные ракурсы, освещение, качество.
- Проверка работы сервиса при высокой нагрузке.


### Оптимизация:
- Оптимизация скорости работы сервиса: время ответа API, скорость загрузки изображений и отображения результатов.


### Мониторинг:
- Настройка мониторинга работы сервиса для отслеживания метрик: количество запросов, время ответа, ошибки.
- Сбор обратной связи.


### Техническая поддержка:
Обеспечить техническую поддержку пользователям сервиса.
Разработать систему обработки ошибок и решения проблем.

---
<p>&nbsp;</p>

## 5. Развитие проекта

Таким образом, для реализации проекта с более точечными настройками потребуется переход от Roboflow к собственной инфрастуктуре, что увеличит сложность, но предоставит больше контроля и гибкости для настройки системы.


### Откроются новые возможности:
- Увеличение количества определяемых стилей: постепенно расширять датасет и обучать модель распознавать больше архитектурных стилей.
- Локализация модели: создать отдельные модели для разных регионов и стран, учитывая особенности местной архитектуры.
- Повышение точности модели: проводить постоянную работу над улучшением качества данных, экспериментировать с новыми архитектурами моделей и методами обучения.
- Добавление новых функций: интегрировать сервис с другими приложениями, разрабатывать новые инструменты для анализа архитектурных стилей, создавать персонализированные рекомендации для пользователей.


### Потребуются ресурсы и технологии:
1. Инфраструктура:
- Сервер или облачное хранилище для данных и моделей (с GPU для обучения, например, Google Cloud, AWS, Yandex Cloud).
- Инструменты для управления данными и версионирования.
2. Инструменты разметки:
- Ручная разметка: LabelImg, CVAT, makesense.ai.
- Автоматизация: Amazon SageMaker Ground Truth, собственные скрипты для предобработки.
3. Программное обеспечение:
- Фреймворки: TensorFlow или PyTorch для обучения и развертывания.
- Библиотеки: OpenCV, PIL для обработки изображений.
- Инструменты для API: FastAPI, Flask, Django.
4. Команда:
- ML-специалист: выбор модели, обучение, оценка.
- Разработчик: API и развертывание.
- Архитектор: проверка данных, выбор датасета.
5. Расходы:
- Облачные ресурсы, лицензии, зарплата специалистов.

---

<p>&nbsp;</p>

# 4. Будущее проекта

Также в будщем возможна доработка функционала и расширение возможностей, в зависимости от того, какие потребности пользователей будут нами отмечены и какими ресурсами команда будет располагать. 

Пользовательский опыт может быть существенно улучшен с помощью добавления голосовых функций, геолокации и дополненной реальности (AR), анализа стилей, рекомендаций и трендов, создания сообщества и социальных интеграций. Могут быть реализованы следующие идеи, которые сделают модель более функциональной и удобной для пользователей, интерактивной и образовательной, и, как следствие, более конкурентоспособной.

### 1. Интерактивный голосовой помощник
Пользователь может голосом или текстом задавать уточняющие вопросы, загружать фото здания, а модель голосом рассказывает:
- архитектурный стиль,
- краткую историческую справку,
- интересные факты о стиле.

Пример: "А когда был популярен этот стиль?", "Какие здания в мире построены в этом стиле?".

Технологии:
- Синтез речи: gTTS, Google Cloud Text-to-Speech или OpenAI TTS.
- Обработка естественного языка (NLP): Dialogflow, Rasa, или transformers (например, GPT для ответов).

### 2. Геолокация и дополненная реальность (AR)
Определение стиля здания на фото с привязкой к геолокации пользователя.

В дополненной реальности (AR) можно отображать дополнительную информацию:
- выделение контуров здания с пояснениями по элементам стиля,
- виртуальные метки с историей строительства, архитекторами и интересными фактами.

Пример использования: Наведение камеры смартфона на здание — AR-помощник "подсвечивает" элементы стиля с пояснениями.

Технологии:
- Геолокация: встроенные функции смартфона, API Google Maps.
- AR: ARCore (Android), ARKit (iOS).

### 3. Историческая реконструкция стилей
Модель может предсказать, как выглядело здание в прошлом или как оно бы выглядело в другом архитектурном стиле.

Пример: Пользователь загружает фото здания в стиле "готика", а модель визуализирует его в стиле "модерн" или "барокко".

Технологии:
- Генеративные модели: Stable Diffusion, StyleGAN, ControlNet.
- Использование fine-tuning на изображениях архитектурных стилей.

### 4. Интерактивный тур по городам и стилям
Создание интерактивного туристического маршрута на основе архитектурных стилей в конкретном городе.
- Определение текущего местоположения пользователя.
- Предложение маршрута с учётом ближайших зданий интересующих стилей.
- Интерактивные подсказки и голосовые экскурсии.

Технологии:
- Геолокация: Google Maps API.
- Создание маршрутов: GraphHopper, OpenRouteService.

### 5. Рекомендательная система для архитектурных любителей
Модель может предлагать пользователям:
- другие здания, выполненные в аналогичном стиле,
- интересные достопримечательности и маршруты по темам,
- книги, статьи или онлайн-экскурсии по архитектуре.

Пример: Если пользователь интересуется "готикой", система порекомендует известные соборы или музеи, выполненные в этом стиле.

Технологии:
- Рекомендательные алгоритмы: Content-based (на основе данных о стилях), Collaborative Filtering.

### 6. Обучающая функция по архитектуре
Интерактивные викторины и тесты на знание архитектурных стилей, где модель задаёт вопросы и анализирует загруженные изображения.
- Модель предлагает пользователю фото здания и спрашивает: "Какой это стиль?".
- Пользователь загружает фото, а модель объясняет, какие элементы указывают на определённый стиль.

Целевая аудитория: студенты-архитекторы, туристы, любители архитектуры.

Технологии:
- Подключение базы данных изображений и текстов.
- Модуль для тестов на Python (например, Flask API с frontend на React или Vue.js).

### 7. Анализ качества изображения и стиля
Модель проверяет фотографии зданий на соответствие выбранному стилю. Полезно для архитекторов и дизайнеров.
- Проверка, насколько элементы здания соответствуют "чистому" стилю.
- Анализ и рекомендации по улучшению проекта здания в выбранном стиле.

Пример: "На фото присутствуют элементы барокко, но для более аутентичного стиля добавьте лепнину на фасаде".

Технологии:
- Классификация и выделение объектов: YOLOv5 + OpenCV.
- NLP для генерации рекомендаций.

### 8. Анализ трендов в архитектуре
Модель на основе больших данных анализирует тренды и популярность архитектурных стилей в разных регионах мира.
- Генерация отчётов по регионам и эпохам.
- Интерактивные визуализации с картами и графиками.

Технологии:
- Анализ данных: Pandas, Plotly, Seaborn.
- Географические данные: GeoPandas, Mapbox.

### 9. Интеграция с социальными сетями
Возможность загружать фото зданий напрямую из соцсетей и делиться результатами анализа.
- Быстрая загрузка изображений из Instagram, Facebook и других платформ.
- Автоматическая публикация результатов анализа с кратким описанием стиля и историей.

Технологии:
- Интеграция через API соцсетей (Instagram API, Facebook API).

### 10. Многопользовательский режим и сообщество
Создание онлайн-сообщества любителей архитектуры.
- Возможность обсуждать результаты распознавания стилей.
- Оценка и голосование за фото зданий по стилям.
- Совместное создание коллекций и маршрутов.

Технологии:
- Разработка социальной платформы: Django, React, Node.js.
